{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import ast\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_stochastic_responses = 20\n",
    "df = pd.DataFrame()\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std_deviation(lst):\n",
    "    # Assign numerical values to categories\n",
    "    mapping = {'A': 1, 'B': 2, 'C': 3, None: 0}\n",
    "    numerical_values = []\n",
    "    for item in lst:\n",
    "        try:\n",
    "            numerical_values.append(mapping[item])\n",
    "        except KeyError:\n",
    "            numerical_values.append(0)\n",
    "    return np.std(numerical_values)\n",
    "\n",
    "def response_parser(s):\n",
    "    match = re.search(r'[A-Z]', s)\n",
    "    if match:\n",
    "        if(match.group(0) not in ['A', 'B', 'C']):\n",
    "            return None\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "\n",
    "def get_most_frequent_value(arr):\n",
    "    counter = Counter(arr)\n",
    "    try:\n",
    "        most_frequent_value, _ = counter.most_common(1)[0]\n",
    "    except:\n",
    "        most_frequent_value = None\n",
    "    return most_frequent_value  \n",
    "\n",
    "def get_most_frequent_non_none_value(arr):\n",
    "    filtered_arr = [x for x in arr if x is not None]\n",
    "    counter = Counter(filtered_arr)\n",
    "    \n",
    "    try:\n",
    "        most_common_value, _ = counter.most_common(1)[0]\n",
    "    except:\n",
    "        most_common_value = None # case when all values in arr are nan\n",
    "    return most_common_value\n",
    "\n",
    "\n",
    "def get_accuracy(pred, true):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred (list): list of lists of stochastic responses\n",
    "        true (list): list of lists of stochastic responses\n",
    "\n",
    "    Returns:\n",
    "        (float) : exact match accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for x, y in zip(pred, true):\n",
    "        if(x == y): count += 1\n",
    "    return count / len(pred)\n",
    "\n",
    "\n",
    "def get_accuracy_none(pred, true):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred (list): list of lists of stochastic responses\n",
    "        true (list): list of lists of stochastic responses\n",
    "\n",
    "    Returns:\n",
    "        (float) : exact match accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for x, y in zip(pred, true):\n",
    "        if(x == None and y == None):\n",
    "            continue\n",
    "        if(x == y): count += 1\n",
    "    return count / len(pred)\n",
    "\n",
    "\n",
    "def get_major_stochastic_response(stochastic_response, consider_nan = False):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        stochastic_response (list): list of lists of stochastic responses\n",
    "\n",
    "    Returns:\n",
    "        (list): major stochastic response across all queries\n",
    "    \"\"\"\n",
    "    major_stochastic_response = []\n",
    "    for arr in stochastic_response:\n",
    "        if(consider_nan):\n",
    "            try:\n",
    "                major_stochastic_response.append(get_most_frequent_value(list(ast.literal_eval(arr))))\n",
    "            except:\n",
    "                major_stochastic_response.append(get_most_frequent_value(arr))\n",
    "            continue\n",
    "        major_stochastic_response.append(get_most_frequent_non_none_value(list(ast.literal_eval(arr))))\n",
    "        \n",
    "    return major_stochastic_response\n",
    "\n",
    "def get_none_value_perc(arr):\n",
    "    s = 0\n",
    "    for elem in arr:\n",
    "        if(elem is None): s += 1\n",
    "    return s / len(arr)\n",
    "\n",
    "def get_none_value_from_response(stochastic_response):\n",
    "    none_perc_response = []\n",
    "    for arr in stochastic_response:\n",
    "        none_perc_response.append(get_none_value_perc(list(ast.literal_eval(arr))))\n",
    "    return none_perc_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '/home/prasoon/snap/main/mtp/llm-science-miscommunication/results/inference'\n",
    "for model_file in os.listdir(output_directory):\n",
    "    models.append(model_file[:-4])\n",
    "    with open(output_directory + '/' + model_file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    model_main = []\n",
    "    model_stochastic = []\n",
    "\n",
    "    for index in tqdm(range(len(result))):\n",
    "        response = result[index]\n",
    "        main_response = response_parser(response['main']['generated_text'])\n",
    "        model_main.append(main_response)\n",
    "        stochastic_responses = []\n",
    "        for stochastic_index in range(number_of_stochastic_responses):\n",
    "            stochastic_responses.append(response_parser(response['stochastic_'+str(stochastic_index)]['generated_text']))\n",
    "        model_stochastic.append(stochastic_responses)\n",
    "    df[model_file[:-4] + '_main'] = model_main\n",
    "    df[model_file[:-4] + '_stochastic'] = model_stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/prasoon/snap/main/mtp/llm-science-miscommunication/data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "subject_wise_results = {'physics': {}, 'mathematics': {}, 'chemistry': {}, 'theoretical_cs': {}}\n",
    "\n",
    "# target keys <- information\n",
    "keys = [    'main_response_accuracy', \n",
    "            'major_stochastic_response_accuracy',\n",
    "            'mean_variance_stochastic_response',\n",
    "            'open_answer_abstinence',\n",
    "            'closed_main_response_accuracy',\n",
    "            'closed_major_stochastic_response_accuracy',\n",
    "            'main_response_stochastic_response_agreeability',\n",
    "            'main_response_stochastic_response_agreeability_without_none']\n",
    "\n",
    "closed_answer_index = data.index[data['answer'] != 'C'].tolist()\n",
    "open_answer_index = data.index[data['answer'] == 'C'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model_results[model] = {}\n",
    "    # collecting the main_response and replacing nan with None\n",
    "    # .... to check the agreeability of the main response and major stochastic responses\n",
    "    main_response = list(df[model + '.pkl_main'])\n",
    "    for index in range(len(main_response)):\n",
    "        try:\n",
    "            if(math.isnan(main_response[index])):\n",
    "                main_response[index] = None\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # collecting the major stochastic responses\n",
    "    #.... considering nan values as value entries and replacing wherever they are major to 'None'\n",
    "    \n",
    "    stochastic_response = list(df[model + '.pkl_stochastic'])\n",
    "    major_stochastic_response = get_major_stochastic_response(stochastic_response, consider_nan = True) \n",
    "    # getting 'main_response_accuracy' & 'stochastic_response_accuracy'\n",
    "    main_response_accuracy = get_accuracy(main_response, data['answer'])\n",
    "    major_stochastic_response_accuracy = get_accuracy(major_stochastic_response, data['answer'])\n",
    "        \n",
    "    # getting 'agreeability' of the main_response and stochastic_response\n",
    "    main_response_stochastic_response_agreeability = get_accuracy(main_response, major_stochastic_response)\n",
    "    main_response_stochastic_response_agreeability_without_none = get_accuracy_none(main_response, major_stochastic_response)\n",
    "    # here we are discounting those cases where both main_response and major_stochastic_response is none\n",
    "    # we are treating them as mismatch\n",
    "    \n",
    "    closed_main_response = [main_response[index] for index in closed_answer_index]\n",
    "    closed_major_stochastic_response = [major_stochastic_response[index] for index in closed_answer_index]  \n",
    "    closed_true_answers = [list(data['answer'])[index] for index in closed_answer_index]\n",
    "    \n",
    "    closed_main_response_accuracy = get_accuracy(closed_main_response, closed_true_answers)\n",
    "    closed_major_stochastic_response_accuracy = get_accuracy(closed_major_stochastic_response, closed_true_answers)\n",
    "    \n",
    "    open_main_response = [main_response[index] for index in open_answer_index]\n",
    "    open_major_stochastic_response = [major_stochastic_response[index] for index in open_answer_index]\n",
    "    open_true_answers = [list(data['answer'])[index] for index in open_answer_index]\n",
    "    \n",
    "    open_main_response_accuracy = get_accuracy(open_main_response, open_true_answers)\n",
    "    open_major_stochastic_response_accuracy = get_accuracy(open_major_stochastic_response, open_true_answers)\n",
    "    \n",
    "    \n",
    "    variance_stochastic_response = []\n",
    "    for arr in stochastic_response:\n",
    "        try:\n",
    "            variance_stochastic_response.append(calculate_std_deviation(ast.literal_eval(arr)))\n",
    "        except:\n",
    "            variance_stochastic_response.append(calculate_std_deviation(arr))\n",
    "        \n",
    "    mean_variance_stochastic_response = np.mean(variance_stochastic_response)\n",
    "    \n",
    "    \n",
    "    model_results[model]['mean_variance_stochastic_response'] = mean_variance_stochastic_response\n",
    "    model_results[model]['main_response_accuracy'] = main_response_accuracy\n",
    "    model_results[model]['major_stochastic_response_accuracy'] = major_stochastic_response_accuracy\n",
    "    model_results[model]['main_response_stochastic_response_agreeability'] = main_response_stochastic_response_agreeability\n",
    "    model_results[model]['main_response_stochastic_response_agreeability_without_none'] = main_response_stochastic_response_agreeability_without_none\n",
    "    model_results[model]['closed_main_response_accuracy'] = closed_main_response_accuracy\n",
    "    model_results[model]['closed_major_stochastic_response_accuracy'] = closed_major_stochastic_response_accuracy\n",
    "    model_results[model]['open_main_response_accuracy'] = open_main_response_accuracy\n",
    "    model_results[model]['open_major_stochastic_response_accuracy'] = open_major_stochastic_response_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
